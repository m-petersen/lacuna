{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd39706",
   "metadata": {},
   "source": [
    "# Functional Lesion Network Mapping (fLNM) - Comprehensive Tutorial\n",
    "\n",
    "This notebook demonstrates all the functionality of the **Functional Lesion Network Mapping** (fLNM) analysis in the Lesion Decoding Toolkit.\n",
    "\n",
    "## What is fLNM?\n",
    "\n",
    "Functional Lesion Network Mapping identifies brain-wide functional connectivity disruptions associated with a focal lesion by:\n",
    "1. Extracting the lesion's timeseries from a normative connectome\n",
    "2. Computing correlations with all other brain voxels\n",
    "3. Producing a **correlation map** (r-map) showing connectivity strength\n",
    "4. Optionally computing **t-statistic maps** and **binary threshold maps**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. âœ… Basic single-subject analysis\n",
    "2. âœ… Both methods: BOES and PINI\n",
    "3. âœ… Batch processing multiple lesions\n",
    "4. âœ… Memory-efficient processing with large connectomes\n",
    "5. âœ… Computing t-statistics and thresholding\n",
    "6. âœ… Interpreting and visualizing results\n",
    "7. âœ… Performance optimization tips\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681777e7",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Create Test Data\n",
    "\n",
    "First, we'll import the necessary libraries and create some mock data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1163aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LDK imports\n",
    "from lacuna import LesionData\n",
    "from lacuna.analysis import FunctionalNetworkMapping\n",
    "from lacuna.batch import batch_process\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801fdd6",
   "metadata": {},
   "source": [
    "### Create Mock Connectome Data\n",
    "\n",
    "We'll create a realistic mock connectome with:\n",
    "- 100 subjects\n",
    "- 100 timepoints per subject\n",
    "- 50,000 voxels (typical for 2mm MNI152 space)\n",
    "- Split into multiple batches to demonstrate batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory for our demo\n",
    "demo_dir = Path(tempfile.mkdtemp(prefix=\"flnm_demo_\"))\n",
    "connectome_dir = demo_dir / \"connectome_batches\"\n",
    "connectome_dir.mkdir()\n",
    "lesion_dir = demo_dir / \"lesions\"\n",
    "lesion_dir.mkdir()\n",
    "output_dir = demo_dir / \"results\"\n",
    "output_dir.mkdir()\n",
    "\n",
    "print(f\"Demo directory: {demo_dir}\")\n",
    "print(f\"Connectome directory: {connectome_dir}\")\n",
    "print(f\"Lesion directory: {lesion_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe587ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic connectome parameters\n",
    "np.random.seed(42)\n",
    "\n",
    "n_voxels = 10000  # Reduced for demo speed (typical: 50,000)\n",
    "n_timepoints = 100\n",
    "n_subjects_per_batch = 20\n",
    "n_batches = 5  # Total: 100 subjects\n",
    "\n",
    "# Create realistic brain mask indices (3D coordinates)\n",
    "# Simulate voxels distributed across the brain\n",
    "x_coords = np.random.randint(10, 80, size=n_voxels)\n",
    "y_coords = np.random.randint(10, 99, size=n_voxels)\n",
    "z_coords = np.random.randint(10, 80, size=n_voxels)\n",
    "mask_indices = np.array([x_coords, y_coords, z_coords])\n",
    "\n",
    "# MNI152 2mm affine matrix\n",
    "mask_affine = np.array(\n",
    "    [[-2.0, 0.0, 0.0, 90.0], [0.0, 2.0, 0.0, -126.0], [0.0, 0.0, 2.0, -72.0], [0.0, 0.0, 0.0, 1.0]]\n",
    ")\n",
    "\n",
    "mask_shape = (91, 109, 91)\n",
    "\n",
    "print(f\"Creating {n_batches} connectome batches...\")\n",
    "print(f\"  - {n_subjects_per_batch} subjects per batch\")\n",
    "print(f\"  - {n_timepoints} timepoints\")\n",
    "print(f\"  - {n_voxels} voxels\")\n",
    "\n",
    "# Create connectome batch files\n",
    "for batch_idx in range(n_batches):\n",
    "    batch_path = connectome_dir / f\"batch_{batch_idx:03d}.h5\"\n",
    "\n",
    "    # Generate realistic timeseries with some structure\n",
    "    # Add correlated noise to simulate real fMRI data\n",
    "    base_signal = np.random.randn(n_timepoints, n_voxels)\n",
    "    timeseries = np.zeros((n_subjects_per_batch, n_timepoints, n_voxels))\n",
    "\n",
    "    for subj in range(n_subjects_per_batch):\n",
    "        # Each subject gets base signal + individual variation\n",
    "        subj_signal = base_signal + np.random.randn(n_timepoints, n_voxels) * 0.5\n",
    "        timeseries[subj] = subj_signal\n",
    "\n",
    "    # Save to HDF5\n",
    "    with h5py.File(batch_path, \"w\") as f:\n",
    "        f.create_dataset(\"timeseries\", data=timeseries.astype(np.float32))\n",
    "        f.create_dataset(\"mask_indices\", data=mask_indices)\n",
    "        f.create_dataset(\"mask_affine\", data=mask_affine)\n",
    "        f.attrs[\"mask_shape\"] = mask_shape\n",
    "\n",
    "    print(f\"  âœ“ Created batch {batch_idx + 1}/{n_batches}\")\n",
    "\n",
    "print(f\"\\nâœ… Connectome data created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09279a",
   "metadata": {},
   "source": [
    "### Create Mock Lesions\n",
    "\n",
    "Now let's create several lesions with different sizes and locations to demonstrate the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f930c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple lesions for testing\n",
    "lesion_specs = [\n",
    "    {\"name\": \"small_frontal\", \"location\": (45, 65, 45), \"size\": 3},\n",
    "    {\"name\": \"medium_parietal\", \"location\": (35, 50, 55), \"size\": 5},\n",
    "    {\"name\": \"large_temporal\", \"location\": (55, 45, 35), \"size\": 7},\n",
    "]\n",
    "\n",
    "lesions = []\n",
    "\n",
    "for spec in lesion_specs:\n",
    "    # Create lesion volume\n",
    "    lesion_data = np.zeros(mask_shape, dtype=np.uint8)\n",
    "\n",
    "    x, y, z = spec[\"location\"]\n",
    "    s = spec[\"size\"]\n",
    "\n",
    "    # Create cubic lesion\n",
    "    lesion_data[x - s : x + s, y - s : y + s, z - s : z + s] = 1\n",
    "\n",
    "    # Create NIfTI image\n",
    "    lesion_img = nib.Nifti1Image(lesion_data, mask_affine)\n",
    "\n",
    "    # Save to file\n",
    "    lesion_path = lesion_dir / f\"{spec['name']}.nii.gz\"\n",
    "    nib.save(lesion_img, lesion_path)\n",
    "\n",
    "    # Load with LesionData\n",
    "    lesion = LesionData.from_nifti(\n",
    "        str(lesion_path),\n",
    "        metadata={\n",
    "            \"space\": \"MNI152_2mm\",\n",
    "            \"subject_id\": spec[\"name\"],\n",
    "            \"lesion_volume_mm3\": int(np.sum(lesion_data) * 8),  # 2mm^3 voxels\n",
    "        },\n",
    "    )\n",
    "    lesions.append(lesion)\n",
    "\n",
    "    print(f\"âœ“ Created lesion: {spec['name']}\")\n",
    "    print(f\"    Location: {spec['location']}, Size: {spec['size']}\")\n",
    "    print(f\"    Volume: {lesion.metadata['lesion_volume_mm3']} mmÂ³\")\n",
    "\n",
    "print(f\"\\nâœ… Created {len(lesions)} lesions for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74b347",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 1: Basic Single-Subject Analysis (BOES Method)\n",
    "\n",
    "The simplest use case: analyze one lesion using the **BOES method** (mean timeseries across all lesion voxels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis object\n",
    "analysis_boes = FunctionalNetworkMapping(\n",
    "    connectome_path=str(connectome_dir),\n",
    "    method=\"boes\",  # Mean timeseries (default)\n",
    "    verbose=True,  # Show progress\n",
    "    compute_t_map=False,  # Skip t-statistics for now\n",
    ")\n",
    "\n",
    "print(\"Analysis Configuration:\")\n",
    "print(f\"  Method: {analysis_boes.method}\")\n",
    "print(f\"  Connectome: {analysis_boes.connectome_path}\")\n",
    "print(f\"  Batch strategy: {analysis_boes.batch_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ababf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on single lesion\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RUNNING SINGLE-SUBJECT ANALYSIS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "result = analysis_boes.run(lesions[0])\n",
    "\n",
    "print(\"\\nâœ… Analysis complete!\")\n",
    "print(f\"Results available in: {result.results.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a6dff",
   "metadata": {},
   "source": [
    "### Inspect Results\n",
    "\n",
    "Let's look at what the analysis produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d119ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fLNM results\n",
    "flnm_results = result.results[\"FunctionalNetworkMapping\"]\n",
    "\n",
    "print(\"Available outputs:\")\n",
    "for key in flnm_results.keys():\n",
    "    value = flnm_results[key]\n",
    "    if isinstance(value, nib.Nifti1Image):\n",
    "        print(f\"  â€¢ {key}: NIfTI image, shape {value.shape}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"  â€¢ {key}: Dictionary with {len(value)} entries\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {key}: {type(value).__name__}\")\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for key, value in flnm_results[\"summary_statistics\"].items():\n",
    "    print(f\"  â€¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677f1b7",
   "metadata": {},
   "source": [
    "### Visualize Correlation Map\n",
    "\n",
    "Let's visualize the functional connectivity pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract correlation map\n",
    "correlation_map = flnm_results[\"correlation_map\"].get_fdata()\n",
    "\n",
    "# Plot 3 orthogonal slices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Axial slice (z)\n",
    "z_slice = 45\n",
    "im1 = axes[0].imshow(\n",
    "    correlation_map[:, :, z_slice].T, cmap=\"RdBu_r\", vmin=-0.5, vmax=0.5, origin=\"lower\"\n",
    ")\n",
    "axes[0].set_title(f\"Axial Slice (z={z_slice})\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Coronal slice (y)\n",
    "y_slice = 54\n",
    "im2 = axes[1].imshow(\n",
    "    correlation_map[:, y_slice, :].T, cmap=\"RdBu_r\", vmin=-0.5, vmax=0.5, origin=\"lower\"\n",
    ")\n",
    "axes[1].set_title(f\"Coronal Slice (y={y_slice})\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Sagittal slice (x)\n",
    "x_slice = 45\n",
    "im3 = axes[2].imshow(\n",
    "    correlation_map[x_slice, :, :].T, cmap=\"RdBu_r\", vmin=-0.5, vmax=0.5, origin=\"lower\"\n",
    ")\n",
    "axes[2].set_title(f\"Sagittal Slice (x={x_slice})\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(\n",
    "    im1, ax=axes, orientation=\"horizontal\", label=\"Correlation (r)\", fraction=0.046, pad=0.04\n",
    ")\n",
    "plt.suptitle(\n",
    "    f\"Functional Connectivity Map: {lesions[0].metadata['subject_id']}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean correlation: {flnm_results['mean_correlation']:.4f}\")\n",
    "print(f\"Correlation range: [{correlation_map.min():.4f}, {correlation_map.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7d9d1",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 2: PINI Method (PCA-based Voxel Selection)\n",
    "\n",
    "The **PINI method** uses PCA to select the most representative lesion voxels, which can improve signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis with PINI method\n",
    "analysis_pini = FunctionalNetworkMapping(\n",
    "    connectome_path=str(connectome_dir),\n",
    "    method=\"pini\",  # PCA-based selection\n",
    "    pini_percentile=75,  # Use top 25% of voxels\n",
    "    verbose=True,\n",
    "    compute_t_map=False,\n",
    ")\n",
    "\n",
    "print(\"PINI Configuration:\")\n",
    "print(f\"  Method: {analysis_pini.method}\")\n",
    "print(f\"  Percentile threshold: {analysis_pini.pini_percentile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PINI analysis on same lesion\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RUNNING PINI ANALYSIS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "result_pini = analysis_pini.run(lesions[0])\n",
    "\n",
    "# Compare with BOES\n",
    "flnm_pini = result_pini.results[\"FunctionalNetworkMapping\"]\n",
    "\n",
    "print(\"\\nComparison: BOES vs PINI\")\n",
    "print(f\"  BOES mean correlation: {flnm_results['mean_correlation']:.4f}\")\n",
    "print(f\"  PINI mean correlation: {flnm_pini['mean_correlation']:.4f}\")\n",
    "print(f\"  Difference: {abs(flnm_pini['mean_correlation'] - flnm_results['mean_correlation']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c551df8",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 3: Computing T-Statistics and Threshold Maps\n",
    "\n",
    "For group-level inference, we can compute **t-statistic maps** to identify significantly connected regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c200336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis with t-statistics\n",
    "analysis_with_t = FunctionalNetworkMapping(\n",
    "    connectome_path=str(connectome_dir),\n",
    "    method=\"boes\",\n",
    "    verbose=True,\n",
    "    compute_t_map=True,  # Compute t-statistics\n",
    "    t_threshold=2.5,  # Threshold at |t| > 2.5\n",
    ")\n",
    "\n",
    "print(\"Configuration with t-statistics:\")\n",
    "print(f\"  Compute t-map: {analysis_with_t.compute_t_map}\")\n",
    "print(f\"  t-threshold: {analysis_with_t.t_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54573497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "result_t = analysis_with_t.run(lesions[1])  # Use medium-sized lesion\n",
    "\n",
    "flnm_t = result_t.results[\"FunctionalNetworkMapping\"]\n",
    "\n",
    "print(\"\\nResults with t-statistics:\")\n",
    "print(f\"  â€¢ t_map: shape {flnm_t['t_map'].shape}\")\n",
    "print(f\"  â€¢ t_threshold_map: shape {flnm_t['t_threshold_map'].shape}\")\n",
    "\n",
    "print(\"\\nT-statistic summary:\")\n",
    "print(f\"  â€¢ t-min: {flnm_t['summary_statistics']['t_min']:.2f}\")\n",
    "print(f\"  â€¢ t-max: {flnm_t['summary_statistics']['t_max']:.2f}\")\n",
    "print(f\"  â€¢ Significant voxels (|t| > 2.5): {flnm_t['summary_statistics']['n_significant_voxels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7efead",
   "metadata": {},
   "source": [
    "### Visualize T-Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61108ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract maps\n",
    "t_map = flnm_t[\"t_map\"].get_fdata()\n",
    "threshold_map = flnm_t[\"t_threshold_map\"].get_fdata()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# T-map slices\n",
    "z_slice = 45\n",
    "axes[0, 0].imshow(t_map[:, :, z_slice].T, cmap=\"RdBu_r\", vmin=-5, vmax=5, origin=\"lower\")\n",
    "axes[0, 0].set_title(f\"T-map: Axial (z={z_slice})\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "y_slice = 54\n",
    "axes[0, 1].imshow(t_map[:, y_slice, :].T, cmap=\"RdBu_r\", vmin=-5, vmax=5, origin=\"lower\")\n",
    "axes[0, 1].set_title(f\"T-map: Coronal (y={y_slice})\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "x_slice = 45\n",
    "axes[0, 2].imshow(t_map[x_slice, :, :].T, cmap=\"RdBu_r\", vmin=-5, vmax=5, origin=\"lower\")\n",
    "axes[0, 2].set_title(f\"T-map: Sagittal (x={x_slice})\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "\n",
    "# Threshold map slices\n",
    "axes[1, 0].imshow(threshold_map[:, :, z_slice].T, cmap=\"hot\", origin=\"lower\")\n",
    "axes[1, 0].set_title(f\"Significant Voxels: Axial\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "axes[1, 1].imshow(threshold_map[:, y_slice, :].T, cmap=\"hot\", origin=\"lower\")\n",
    "axes[1, 1].set_title(f\"Significant Voxels: Coronal\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "axes[1, 2].imshow(threshold_map[x_slice, :, :].T, cmap=\"hot\", origin=\"lower\")\n",
    "axes[1, 2].set_title(f\"Significant Voxels: Sagittal\")\n",
    "axes[1, 2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"T-Statistics: {result_t.metadata['subject_id']}\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004476d2",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 4: Batch Processing Multiple Lesions\n",
    "\n",
    "Process all lesions efficiently using **vectorized batch processing** - the recommended method for multiple subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c99fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis for batch processing\n",
    "analysis_batch = FunctionalNetworkMapping(\n",
    "    connectome_path=str(connectome_dir),\n",
    "    method=\"boes\",\n",
    "    verbose=True,\n",
    "    compute_t_map=True,\n",
    "    t_threshold=2.0,\n",
    ")\n",
    "\n",
    "print(\"Batch Processing Configuration:\")\n",
    "print(f\"  Number of lesions: {len(lesions)}\")\n",
    "print(f\"  Strategy: vectorized (automatic)\")\n",
    "print(f\"  Compute t-maps: Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all lesions at once\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BATCH PROCESSING ALL LESIONS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "results_batch = batch_process(\n",
    "    lesions,\n",
    "    analysis_batch,\n",
    "    strategy=\"vectorized\",  # Vectorized = fastest for multiple lesions\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(results_batch)} lesions successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2ea7c",
   "metadata": {},
   "source": [
    "### Compare Results Across Lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eda4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for result in results_batch:\n",
    "    flnm = result.results[\"FunctionalNetworkMapping\"]\n",
    "    stats = flnm[\"summary_statistics\"]\n",
    "\n",
    "    comparison_data.append(\n",
    "        {\n",
    "            \"Subject\": result.metadata[\"subject_id\"],\n",
    "            \"Mean r\": stats[\"mean\"],\n",
    "            \"Std r\": stats[\"std\"],\n",
    "            \"Max r\": stats[\"max\"],\n",
    "            \"Min r\": stats[\"min\"],\n",
    "            \"t-max\": stats.get(\"t_max\", np.nan),\n",
    "            \"t-min\": stats.get(\"t_min\", np.nan),\n",
    "            \"Sig. voxels\": stats.get(\"n_significant_voxels\", 0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nBatch Processing Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c76bb0",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 5: Memory-Efficient Processing with Lesion Batching\n",
    "\n",
    "For very large numbers of lesions, process them in smaller batches to control memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback to save results incrementally\n",
    "def save_batch_results(batch_results):\n",
    "    \"\"\"Save results after each batch to free memory.\"\"\"\n",
    "    for result in batch_results:\n",
    "        subject_id = result.metadata[\"subject_id\"]\n",
    "        flnm = result.results[\"FunctionalNetworkMapping\"]\n",
    "\n",
    "        # Save correlation map\n",
    "        rmap_path = output_dir / f\"{subject_id}_rmap.nii.gz\"\n",
    "        nib.save(flnm[\"correlation_map\"], rmap_path)\n",
    "\n",
    "        # Save t-map if available\n",
    "        if \"t_map\" in flnm:\n",
    "            tmap_path = output_dir / f\"{subject_id}_tmap.nii.gz\"\n",
    "            nib.save(flnm[\"t_map\"], tmap_path)\n",
    "\n",
    "        print(f\"    ðŸ’¾ Saved: {subject_id}\")\n",
    "\n",
    "\n",
    "print(\"Memory-Efficient Batch Processing:\")\n",
    "print(f\"  Lesion batch size: 2\")\n",
    "print(f\"  Total lesions: {len(lesions)}\")\n",
    "print(f\"  Expected batches: {(len(lesions) + 1) // 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae63b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with memory control\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MEMORY-EFFICIENT BATCH PROCESSING\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "results_memory_efficient = batch_process(\n",
    "    lesions,\n",
    "    analysis_batch,\n",
    "    strategy=\"vectorized\",\n",
    "    lesion_batch_size=2,  # Process 2 lesions at a time\n",
    "    batch_result_callback=save_batch_results,  # Save immediately\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: {output_dir}\")\n",
    "print(f\"âœ… Files created:\")\n",
    "for file in sorted(output_dir.glob(\"*.nii.gz\")):\n",
    "    print(f\"    â€¢ {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd5442",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 6: Performance Monitoring and Timing\n",
    "\n",
    "Monitor batch processing performance to optimize your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Time the processing\n",
    "print(\"Performance Test: Processing 3 lesions\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results_timed = batch_process(\n",
    "    lesions,\n",
    "    analysis_batch,\n",
    "    strategy=\"vectorized\",\n",
    "    show_progress=False,  # Reduce output for timing\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nPerformance Results:\")\n",
    "print(f\"  Total time: {total_time:.2f}s\")\n",
    "print(f\"  Time per lesion: {total_time / len(lesions):.2f}s\")\n",
    "print(f\"  Total subjects processed: {100}\")\n",
    "print(f\"  Time per subject: {total_time / 100:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3aa547",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 7: Saving and Loading Results\n",
    "\n",
    "Learn how to save your analysis results and load them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67862e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all outputs for a lesion\n",
    "subject_id = lesions[0].metadata[\"subject_id\"]\n",
    "result_to_save = results_batch[0]\n",
    "flnm_to_save = result_to_save.results[\"FunctionalNetworkMapping\"]\n",
    "\n",
    "print(f\"Saving results for: {subject_id}\")\n",
    "\n",
    "# Save all NIfTI outputs\n",
    "output_files = []\n",
    "\n",
    "# Correlation map\n",
    "rmap_file = output_dir / f\"{subject_id}_correlation_map.nii.gz\"\n",
    "nib.save(flnm_to_save[\"correlation_map\"], rmap_file)\n",
    "output_files.append(rmap_file)\n",
    "\n",
    "# Z-map\n",
    "zmap_file = output_dir / f\"{subject_id}_z_map.nii.gz\"\n",
    "nib.save(flnm_to_save[\"z_map\"], zmap_file)\n",
    "output_files.append(zmap_file)\n",
    "\n",
    "# T-map (if computed)\n",
    "if \"t_map\" in flnm_to_save:\n",
    "    tmap_file = output_dir / f\"{subject_id}_t_map.nii.gz\"\n",
    "    nib.save(flnm_to_save[\"t_map\"], tmap_file)\n",
    "    output_files.append(tmap_file)\n",
    "\n",
    "# Threshold map (if computed)\n",
    "if \"t_threshold_map\" in flnm_to_save:\n",
    "    threshmap_file = output_dir / f\"{subject_id}_threshold_map.nii.gz\"\n",
    "    nib.save(flnm_to_save[\"t_threshold_map\"], threshmap_file)\n",
    "    output_files.append(threshmap_file)\n",
    "\n",
    "print(f\"\\nâœ… Saved {len(output_files)} files:\")\n",
    "for file in output_files:\n",
    "    print(f\"    â€¢ {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcec668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results back\n",
    "print(\"\\nLoading results...\")\n",
    "\n",
    "loaded_rmap = nib.load(str(rmap_file))\n",
    "loaded_zmap = nib.load(str(zmap_file))\n",
    "\n",
    "print(f\"  âœ“ Loaded correlation map: {loaded_rmap.shape}\")\n",
    "print(f\"  âœ“ Loaded z-map: {loaded_zmap.shape}\")\n",
    "\n",
    "# Verify data integrity\n",
    "original_rmap_data = flnm_to_save[\"correlation_map\"].get_fdata()\n",
    "loaded_rmap_data = loaded_rmap.get_fdata()\n",
    "\n",
    "match = np.allclose(original_rmap_data, loaded_rmap_data, rtol=1e-5)\n",
    "print(f\"\\nâœ… Data integrity check: {'PASSED' if match else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba54d17",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Methods Available\n",
    "1. **BOES** (default): Mean timeseries across all lesion voxels\n",
    "   - Fast, straightforward\n",
    "   - Good for most use cases\n",
    "   \n",
    "2. **PINI**: PCA-based voxel selection\n",
    "   - Can improve signal-to-noise\n",
    "   - Use `pini_percentile` to control selection\n",
    "\n",
    "### Processing Strategies\n",
    "1. **Single subject**: Use `analysis.run(lesion)`\n",
    "2. **Multiple subjects**: Use `batch_process(lesions, analysis)`\n",
    "3. **Memory-constrained**: Use `lesion_batch_size` parameter\n",
    "\n",
    "### Outputs Generated\n",
    "- **correlation_map**: r-values showing connectivity strength\n",
    "- **z_map**: Fisher z-transformed correlations\n",
    "- **t_map**: t-statistics for group inference (optional)\n",
    "- **t_threshold_map**: Binary map of significant voxels (optional)\n",
    "- **summary_statistics**: Mean, std, max, min, etc.\n",
    "\n",
    "### Performance Tips\n",
    "- âœ… Use **vectorized strategy** for multiple lesions (10-50x faster)\n",
    "- âœ… Set `lesion_batch_size` to control memory (recommended: 20-50)\n",
    "- âœ… Use `batch_result_callback` for incremental saving\n",
    "- âœ… Split large connectomes into multiple HDF5 files\n",
    "- âœ… Monitor timing with `verbose=True`\n",
    "\n",
    "### Next Steps\n",
    "- Try with your own lesion data\n",
    "- Experiment with different parameters\n",
    "- Compare BOES vs PINI for your use case\n",
    "- Visualize results with your preferred tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61abb1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "Clean up the temporary files created during this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7540df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional - comment out to keep files)\n",
    "import shutil\n",
    "\n",
    "# shutil.rmtree(demo_dir)\n",
    "# print(f\"âœ“ Cleaned up demo directory: {demo_dir}\")\n",
    "\n",
    "print(f\"\\nDemo files preserved at: {demo_dir}\")\n",
    "print(\"To remove them, uncomment the cleanup code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f7967",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- **User Guide**: `docs/analysis/functional_network_mapping.md`\n",
    "- **API Reference**: `docs/api/analysis.md`\n",
    "- **Memory Management**: `docs/memory_management.md`\n",
    "- **Batch Processing**: `docs/batch_timing.md`\n",
    "\n",
    "### Example Scripts\n",
    "- **Batch processing**: `examples/batch_processing_example.py`\n",
    "- **Production script**: `test_batch_flnm.py`\n",
    "\n",
    "### Getting Help\n",
    "- **GitHub Issues**: Report bugs or request features\n",
    "- **Discussions**: Ask questions and share experiences\n",
    "\n",
    "Happy analyzing! ðŸ§ âœ¨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
