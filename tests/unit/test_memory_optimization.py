"""
Test memory optimization improvements in vectorized batch processing.

This test validates that the streaming aggregation approach significantly
reduces memory usage compared to accumulating all correlation maps.
"""

import h5py
import nibabel as nib
import numpy as np
import pytest

from lacuna import MaskData
from lacuna.analysis import FunctionalNetworkMapping
from lacuna.assets.connectomes import (
    register_functional_connectome,
    unregister_functional_connectome,
)
from lacuna.batch import batch_process


@pytest.fixture
def mock_connectome_batched(tmp_path):
    """Create mock connectome split into multiple batch files."""
    n_voxels = 125
    x_coords = np.repeat(range(43, 48), 25)
    y_coords = np.tile(np.repeat(range(52, 57), 5), 5)
    z_coords = np.tile(range(43, 48), 25)
    mask_indices = np.array([x_coords, y_coords, z_coords])

    affine = np.array(
        [
            [-2.0, 0.0, 0.0, 90.0],
            [0.0, 2.0, 0.0, -126.0],
            [0.0, 0.0, 2.0, -72.0],
            [0.0, 0.0, 0.0, 1.0],
        ]
    )

    connectome_dir = tmp_path / "connectomes"
    connectome_dir.mkdir()

    # Create 5 batches of 20 subjects each = 100 total
    for batch_idx in range(5):
        connectome_path = connectome_dir / f"batch_{batch_idx:03d}.h5"
        with h5py.File(connectome_path, "w") as f:
            f.create_dataset("timeseries", data=np.random.randn(20, 100, n_voxels))
            f.create_dataset("mask_indices", data=mask_indices)
            f.create_dataset("mask_affine", data=affine)
            f.attrs["mask_shape"] = (91, 109, 91)

    return {
        "connectome_dir": connectome_dir,
        "affine": affine,
        "mask_shape": (91, 109, 91),
        "n_subjects": 100,
        "n_batches": 5,
    }


@pytest.fixture
def mock_lesions(tmp_path, mock_connectome_batched):
    """Create mock lesions for testing."""
    affine = mock_connectome_batched["affine"]
    lesions = []

    for i in range(10):
        mask_data_array = np.zeros((91, 109, 91), dtype=np.uint8)
        mask_data_array[43:48, 52:57, 43:48] = 1
        mask_img = nib.Nifti1Image(mask_data_array, affine)
        lesion_path = tmp_path / f"lesion_{i}.nii.gz"
        nib.save(mask_img, lesion_path)
        lesion = MaskData.from_nifti(
            str(lesion_path),
            metadata={"space": "MNI152NLin6Asym", "resolution": 2, "subject_id": f"test_{i:03d}"},
        )
        lesions.append(lesion)

    return lesions


def test_streaming_aggregation_produces_correct_results(mock_connectome_batched, mock_lesions):
    """Test that streaming aggregation produces identical results to accumulation."""
    connectome_dir = mock_connectome_batched["connectome_dir"]

    # Register the connectome
    register_functional_connectome(
        name="test_streaming_connectome",
        space="MNI152NLin6Asym",
        resolution=2.0,
        data_path=connectome_dir,
        n_subjects=100,
        description="Test batched connectome"
    )

    try:
        # Create analysis
        analysis = FunctionalNetworkMapping(
            connectome_name="test_streaming_connectome",
            method="boes",
            log_level=0,
            compute_t_map=True,
            t_threshold=2.0,
        )

        # Process lesions
        results = batch_process(
            mock_lesions[:3],  # Just test first 3 for speed
            analysis,
            strategy="vectorized",
            show_progress=False,
        )

        # Validate results structure
        assert len(results) == 3

        for result in results:
            flnm = result.results["FunctionalNetworkMapping"]

            # Check all expected outputs exist
            assert "CorrelationMap" in flnm
            assert "ZMap" in flnm
            assert "TMap" in flnm
            assert "summary_statistics" in flnm

            # Check shapes - in nested dict, these are VoxelMap objects
            # VoxelMap.data holds the NIfTI image which has .shape
            assert flnm["CorrelationMap"].shape == (91, 109, 91)
            assert flnm["TMap"].shape == (91, 109, 91)

            # Check aggregated across all subjects
            # In batch results, summary_statistics is already the dict data
            summary_data = flnm["summary_statistics"]
            assert summary_data["n_subjects"] == 100
            assert summary_data["n_batches"] == 5

            # Check values in reasonable range
            assert -1 <= summary_data["mean"] <= 1
    finally:
        unregister_functional_connectome("test_streaming_connectome")


def test_streaming_aggregation_with_lesion_batches(mock_connectome_batched, mock_lesions):
    """Test streaming aggregation with lesion batching."""
    connectome_dir = mock_connectome_batched["connectome_dir"]

    register_functional_connectome(
        name="test_lesion_batch_connectome",
        space="MNI152NLin6Asym",
        resolution=2.0,
        data_path=connectome_dir,
        n_subjects=100,
        description="Test batched connectome"
    )

    try:
        saved_batches = []

        def save_callback(batch_results):
            saved_batches.append([r.metadata["subject_id"] for r in batch_results])

        analysis = FunctionalNetworkMapping(
            connectome_name="test_lesion_batch_connectome",
            method="boes",
            log_level=0,
            compute_t_map=True
        )

        # Process with lesion batches of 3
        results = batch_process(
            mock_lesions,
            analysis,
            strategy="vectorized",
            lesion_batch_size=3,
            batch_result_callback=save_callback,
            show_progress=False,
        )

        # Verify all lesions processed
        assert len(results) == 10

        # Verify batches were saved incrementally
        assert len(saved_batches) == 4  # 10 lesions / 3 per batch = 4 batches
        assert saved_batches[0] == ["test_000", "test_001", "test_002"]
        assert saved_batches[1] == ["test_003", "test_004", "test_005"]
        assert saved_batches[2] == ["test_006", "test_007", "test_008"]
        assert saved_batches[3] == ["test_009"]  # Last batch partial

        # Verify all results have correct subject count
        for result in results:
            assert result.results["FunctionalNetworkMapping"]["summary_statistics"]["n_subjects"] == 100
    finally:
        unregister_functional_connectome("test_lesion_batch_connectome")


def test_float32_optimization(mock_connectome_batched, mock_lesions):
    """Test that float32 is used throughout for memory efficiency."""
    connectome_dir = mock_connectome_batched["connectome_dir"]

    register_functional_connectome(
        name="test_float32_connectome",
        space="MNI152NLin6Asym",
        resolution=2.0,
        data_path=connectome_dir,
        n_subjects=100,
        description="Test batched connectome"
    )

    try:
        analysis = FunctionalNetworkMapping(
            connectome_name="test_float32_connectome",
            method="boes",
            log_level=0,
            compute_t_map=False
        )

        results = batch_process(mock_lesions[:2], analysis, strategy="vectorized", show_progress=False)

        # Check that output arrays use float32 internally
        for result in results:
            flnm = result.results["FunctionalNetworkMapping"]

            # Check data type of nibabel images (get_fdata() converts to float64)
            # So we check the internal data type instead
            assert flnm["CorrelationMap"].get_data_dtype() == np.float32
            assert flnm["ZMap"].get_data_dtype() == np.float32
    finally:
        unregister_functional_connectome("test_float32_connectome")


def test_t_statistics_with_streaming(mock_connectome_batched, mock_lesions):
    """Test t-statistic computation with streaming aggregation."""
    connectome_dir = mock_connectome_batched["connectome_dir"]

    register_functional_connectome(
        name="test_tstat_connectome",
        space="MNI152NLin6Asym",
        resolution=2.0,
        data_path=connectome_dir,
        n_subjects=100,
        description="Test batched connectome"
    )

    try:
        analysis = FunctionalNetworkMapping(
            connectome_name="test_tstat_connectome",
            method="boes",
            log_level=0,
            compute_t_map=True,
            t_threshold=2.5,
        )

        results = batch_process(mock_lesions[:2], analysis, strategy="vectorized", show_progress=False)

        for result in results:
            flnm = result.results["FunctionalNetworkMapping"]

            # T-map should exist
            assert "TMap" in flnm
            assert flnm["TMap"].shape == (91, 109, 91)

            # Threshold map should exist
            assert "TThresholdMap" in flnm

            # Statistics should include t-map info
            stats = flnm["summary_statistics"]
            assert "t_min" in stats
            assert "t_max" in stats
            assert "t_threshold" in stats
            assert stats["t_threshold"] == 2.5
            assert "n_significant_voxels" in stats
    finally:
        unregister_functional_connectome("test_tstat_connectome")


def test_memory_efficiency_improvement():
    """
    Conceptual test documenting memory improvements.

    This test documents the expected memory savings from streaming aggregation.
    Actual measurements require profiling tools.
    """
    # Memory comparison (theoretical):
    n_lesions = 20
    n_subjects = 1000
    n_voxels = 50000
    bytes_per_float32 = 4
    bytes_per_float64 = 8

    # OLD APPROACH: Accumulate all correlation maps
    old_memory_r_maps = n_lesions * n_subjects * n_voxels * bytes_per_float32
    old_memory_mb = old_memory_r_maps / (1024 * 1024)

    # NEW APPROACH: Streaming aggregation (only store sums)
    # sum_z + sum_z2 for each lesion
    new_memory_aggregators = n_lesions * n_voxels * 2 * bytes_per_float64
    new_memory_mb = new_memory_aggregators / (1024 * 1024)

    # Calculate reduction
    reduction_factor = old_memory_mb / new_memory_mb
    reduction_percent = (1 - new_memory_mb / old_memory_mb) * 100

    # Document expected improvements
    assert old_memory_mb > 3800, f"Old: {old_memory_mb:.0f} MB"
    assert new_memory_mb < 20, f"New: {new_memory_mb:.0f} MB"  # 15.3 MB for this example
    assert reduction_factor > 200, f"Reduction: {reduction_factor:.0f}x"
    assert reduction_percent > 99, f"Reduction: {reduction_percent:.1f}%"

    print("\nMemory Optimization Benefits:")
    print(f"  Old approach: {old_memory_mb:.0f} MB (accumulate all r_maps)")
    print(f"  New approach: {new_memory_mb:.0f} MB (streaming aggregation)")
    print(f"  Reduction: {reduction_factor:.0f}x ({reduction_percent:.1f}%)")
    print("  \nWith same RAM, can now process:")
    print(f"    {reduction_factor:.0f}x more lesions per batch")
    print(f"    e.g., 20 lesions â†’ {int(20 * reduction_factor)} lesions")
